{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2109006,"sourceType":"datasetVersion","datasetId":1346}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns;sns.set()\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom pandas import read_csv, set_option\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer,RobustScaler\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom catboost import CatBoostClassifier,CatBoostRegressor\nfrom sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\nfrom sklearn.feature_selection import SelectKBest,f_regression\nfrom xgboost import plot_importance,XGBClassifier,XGBRegressor\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn import preprocessing\nimport shap\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import SparsePCA\nfrom sklearn.decomposition import KernelPCA\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import MiniBatchDictionaryLearning\nfrom sklearn.decomposition import FastICA\nfrom sklearn.manifold import Isomap\nfrom sklearn.manifold import MDS\nfrom sklearn.manifold import LocallyLinearEmbedding\nfrom sklearn.manifold import TSNE\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\n\n# for dirname, _, filenames in os.walk('/kaggle/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        \nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set(style='whitegrid')\n%matplotlib inline\n\n# time series cross validation\n# https://hub.packtpub.com/cross-validation-strategies-for-time-series-forecasting-tutorial/\n\n''' FUNCTIONS '''\n\n# One plot type\ndef plot_line(ldf,lst,title='',sec_id=None,size=[350,1000]):\n    \n    # sec_id - list of [False,False,True] values of when to activate supblots; same length as lst\n    \n    if(sec_id is not None):\n        fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n    else:\n        fig = go.Figure()\n        \n    if(len(lst) is not 1):\n        ii=-1\n        for i in lst:\n            ii+=1\n            if(sec_id is not None):\n                fig.add_trace(go.Scatter(x=ldf.index, y=ldf[lst[ii]],mode='lines',name=lst[ii],line=dict(width=2.0)),secondary_y=sec_id[ii])\n            else:\n                fig.add_trace(go.Scatter(x=ldf.index, y=ldf[lst[ii]],mode='lines',name=lst[ii],line=dict(width=2.0)))\n    else:\n        fig.add_trace(go.Scatter(x=ldf.index, y=ldf[lst[0]],mode='lines',name=lst[0],line=dict(width=2.0)))\n\n    fig.update_layout(height=size[0],width=size[1],template='plotly_white',title=title,\n                          margin=dict(l=50,r=80,t=50,b=40));fig.show()\n    \n# plot n verticle subplots\ndef plot_vsubplots(ldf,lst,title='',nplots=None,lw_id=None,size=[400,1000]):\n\n    # lw_id list of line widths if added\n        \n    assert(nplots is not None) \n    fig = make_subplots(rows=nplots,shared_xaxes=True)\n    ii=-1\n    for i in lst:\n        ii+=1\n        fig.add_trace(go.Scatter(x=ldf.index,y=ldf[lst[ii]], mode='lines',name=lst[ii],line=dict(width=lw_id[ii])), row=ii+1, col=1) \n\n    fig.update_layout(height=size[0],width=size[1],template='plotly_white',title=title,\n                          margin=dict(l=50,r=80,t=50,b=40));fig.show()\n    \ncolours = ['tab:blue','tab:red','tab:green']\ndef plot_line2(ldf,lst,title=''):\n    \n    ii=-1\n    plt.figure(figsize=(14,5))\n    for i in lst:\n        ii+=1\n        ax = ldf[lst[ii]].plot(color=colours[ii],label=lst[ii],lw=1.5)\n    plt.title(title)\n    plt.legend();plt.show()\n    \ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Plot Correlation to Target Variable only\ndef corrMat(df,target='demand',figsize=(9,0.5),ret_id=False):\n    \n    corr_mat = df.corr().round(2);shape = corr_mat.shape[0]\n    corr_mat = corr_mat.transpose()\n    corr = corr_mat.loc[:, df.columns == target].transpose().copy()\n    \n    if(ret_id is False):\n        f, ax = plt.subplots(figsize=figsize)\n        sns.heatmap(corr,vmin=-0.3,vmax=0.3,center=0, \n                     cmap=cmap,square=False,lw=2,annot=True,cbar=False)\n        plt.title(f'Feature Correlation to {target}')\n    \n    if(ret_id):\n        return corr\n    \ndef bar_plot(x, y,palette_len,title='Missing Values (%)', xlim = None, ylim = None, \n             xticklabels = None, yticklabels = None,xlabel = None, ylabel = None, \n             figsize = (10,4),axis_grid = 'y'):\n        \n    cmap = sns.color_palette(\"plasma\")\n    fig, ax = plt.subplots(figsize = figsize)\n    plt.title(title,size = 15, fontweight = 'bold')\n\n    for i in ['top', 'right', 'bottom', 'left']:\n        ax.spines[i].set_color('black')\n    \n    ax.spines['top'].set_visible(True);ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False);ax.spines['left'].set_visible(False)\n\n    sns.barplot(x = x, y = y, edgecolor = 'black', ax = ax,\n                palette = cmap)\n    ax.set_xlim(xlim);ax.set_ylim(ylim)    \n    ax.set_xticklabels(xticklabels);ax.set_yticklabels(yticklabels)\n    plt.xlabel(xlabel);plt.ylabel(ylabel)\n    ax.grid(axis = axis_grid,ls='--',alpha = 0.9)\n    plt.show()\n\n# function to plot a two PCA Feature Plot using Pandas \ndef scatterPlot(xDF, yDF, algoName):\n    \n    sns.set_style('whitegrid')\n    fig, ax = plt.subplots()\n    tempDF = pd.DataFrame(data=xDF.loc[:,0:1], index=xDF.index)\n    tempDF = pd.concat((tempDF,yDF), axis=1, join=\"inner\")\n    tempDF.columns = [\"Component 1\",\"Component 2\",\"Label\"]\n    g = sns.scatterplot(x=\"Component 1\",y=\"Component 2\",data=tempDF,hue=\"Label\",\n                        linewidth=0.5,alpha=0.5,s=15,edgecolor='k')\n    plt.title(algoName);plt.legend()\n    \n    for i in ['top', 'right', 'bottom', 'left']:\n        ax.spines[i].set_color('black')\n    \n    ax.spines['top'].set_visible(False);ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False);ax.spines['left'].set_visible(False)\n    ax.grid(axis = 'both',ls='--',alpha = 0.9)\n    plt.show()\n    \n    \n# reduce memory (@mfjwr1); distorts the data a little (but reduces by 60% memory)\ndef red_mem(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\n\n# Split for TimeSeries\ndef TimeSeries_Split(ldf,\n                     split_id=[None,None],\n                     test_id=False,\n                     cut_id=None):\n    \n    # Reduce the number of used data\n    if(cut_id is not None):\n        print('Reducing Input Data')\n        \n        if(type(cut_id) is int):\n            ldf = ldf.iloc[-cut_id:]\n        else:\n            # input anything other than int\n            print('Slicing based on period')\n            ldf = ldf[data_period]\n            \n        t1 = ldf.index.max();t0 = ldf.index.min()\n        print(f'Dataset Min.Index: {t0} | Max.Index: {t1}')\n        \n    if(split_id[0] is not None):\n        # General Percentage Split (Non Shuffle requied for Time Series)\n        train_df,pred_df = train_test_split(ldf,test_size=split_id[0],shuffle=False)\n    elif(split_id[1] is not None):\n        # specific time split \n        train_df = df.loc[:split_id[1]]; pred_df = df.loc[split_id[1]:] \n    else:\n        print('Choose One Splitting Method Only')\n        \n#     y_train = train_df[feature]\n#     X_train = train_df.loc[:, train_df.columns != feature]\n#     if(test_id):\n#         y_test = pred_df[feature]\n#         X_test = pred_df.loc[:, pred_df.columns != feature]\n        \n    return train_df,pred_df # return ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-27T20:17:58.3241Z","iopub.execute_input":"2023-04-27T20:17:58.324644Z","iopub.status.idle":"2023-04-27T20:17:58.423748Z","shell.execute_reply.started":"2023-04-27T20:17:58.324595Z","shell.execute_reply":"2023-04-27T20:17:58.42265Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://i.imgur.com/j5CTpDr.png)\n\n# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#CDE10F\"><b><span style='color:#FFFFFF'>1 |</span></b> <b>INTRODUCTION</b></div>\n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#CDE10F'>FOREWORD</span></b></p></div>\n\n- Recently (2020 alone), a digital asset **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">Bitcoin</mark>** has increased over 200% in value alone, having reached new heights, after which it dropped back down again\n- This shows how **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">volatile</mark>** this asset is, which makes it one of the more interesting time series to explore, as opposed to standard stocks\n- It's quite interesting to see how this **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">digital asset</mark>** has resurged in value after a recent dip, especially in the current financial climate\n- There are quite a number of factors that can affect the asset, and we'll only be looking at traditional stock based factors\n- This notebook is written for my own time series problem practice & coding of generic functions/classes and nothing more, the relevant dataset is used because of the extensive data availability of a highly volitile asset\n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#CDE10F'>DIGITAL ASSETS</span></b></p></div>\n\nSnipplet from **[Dataset Description](https://www.kaggle.com/mczielinski/bitcoin-historical-data)**\n\n> Bitcoin is the longest running and most well known cryptocurrency, first released as open source in 2009 by the anonymous Satoshi Nakamoto. Bitcoin serves as a decentralized medium of digital exchange, with transactions verified and recorded in a public distributed ledger (the blockchain) without the need for a trusted record keeping authority or central intermediary. Transaction blocks contain a SHA-256 cryptographic hash of previous transaction blocks, and are thus \"chained\" together, serving as an immutable record of all transactions that have ever occurred. As with any currency/commodity on the market, bitcoin trading and financial instruments soon followed public adoption of bitcoin and continue to grow. Included here is historical bitcoin market data at 1-min intervals for select bitcoin exchanges where trading takes place. Happy (data) mining! ","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#CDE10F'>STOCK PRICE INFLUENCES</span></b></p></div>\n\nFeatures that can be useful for **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">stock price</mark>** prediction as outlined by <b>Tatsat et al (2020)</b>\n\n##### **CORRELATED ASSETS**\n\n> An organization depends on and interacts with many external factors, including its competitors, clients, the global economy, the geopolitical situation, fiscal and monetary policies, access to capital, and so on. Hence, its stock price may be correlated not only with the stock price of other companies but also with other assets such as commodities, FX, broad-based indices, or even fixed income securities.\n\n##### **TECHNICAL INDICATORS**\n\n> A lot of investors follow technical indicators. Moving average, exponential moving average, and momentum are the most popular indicators. \n\n##### **TECHNICAL REPORTS**\n\n> Annual and quarterly reports of companies can be used to extract or determine key metrics, such as ROE (Return on Equity) and P/E (Price-toEarnings).\n\n##### **NEWS REPORTS**\n\n> News can indicate upcoming events that can potentially move the stock price in a certain direction.\n\n- When focusing on **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">stock price</mark>**, one can utilise a few approaches when it comes to feature building/assembly (factors that affect the predicted variable) as shown above\n- As indicated by **([source](https://cryptobriefing.com/is-bitcoin-stock-commodity/))**, <code>Bitcoin</code> is more of a **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">digital asset</mark>**, than a **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">stock</mark>** or a **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">currency</mark>**, thus it's not quite certain whether factors outlined are completely relevant to analyses involving <code>Bitcoins</code>\n- However, **([source](https://www.mycryptopedia.com/best-8-bitcoin-indicators-for-cryptocurrency-trading/))** does outline various <code>indicators</code> that are useful specifically for <code>Bitcoin</code>price directivity prediction, which is a reasurence that **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">technical indicators</mark>** play and important role in <code>Bitcoin</code> time-series as well","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#CDE10F'>PROBLEM DEFINITION</span></b></p></div>\n\n- A major drawback of crypocurrency trading is the **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">volatility</mark>** of the market.\n- The currency trades can occur 24/7 & tracking crypto position can be an impossible task to manage without automation.\n- Automated Machine Learning trading algorithms can assist in managing this task, in order to predict the market's movement.\n- We can use models to classify future movements into three categries: \n\n> <code>(1) The market will rise (take long position)</code>, <br>\n> <code>(2) The market will fall (take short position)</code> <br>\n> <code>(3) The market will move sideways (take no position)</code>.\n    \n    \n- The problem of predicting a buy (<code>value=1</code>) or sell (<code>value=0</code>) signal for a **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">trading strategy</mark>** is defined in the\nclassification framework. \n- The buy or sell signal are decided on the basis of a comparison of short term vs. long\nterm price & is defined in <code>Section 2.2</code>\n- Data harvesting (just data collection here) & <code>feature engineering</code> are relevant factors in time series model improvement. \n- It's interesting to investigate whether traditionally stock orientated feature engineering modifications are relevant to <code>digital assets</code>, and if so which ones\n- Last but not least, **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">model generation efficiency</mark>** becomes much more significant when dealing with **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">High Frequency Tick Data</mark>** as each added feature can have a substatial impact on the turnaround time of a model, due to the amount of data & balancing model accuracy & model output turnaround time is definitely worth managing","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#CDE10F\"><b><span style='color:#FFFFFF'>2 |</span></b> <b>THE DATASET</b></div>\n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#CDE10F'>READING DATASET</span></b></p></div>\n\n- Current dataset : CSV file for select bitcoin exchanges for the time period of Jan 2012 to September 2020, <code>1-min interval data</code> ([dataset](https://www.kaggle.com/mczielinski/bitcoin-historical-data))\n- The feature <code>timestamp</code> can be parsed into a more conventional time index using the <code>pytz</code> library.\n- The <code>Baseline Features</code> include: the asset's minute's <code>open</code>,<code>high</code>,<code>low</code>,<code>close</code>,<code>Volume_(BTC)</code>,<code>Volume_(Currency)</code> & <code>Weighted_Price</code>\n- The loaded dataset, contains a specific start and end time index, in order to use models on unseen data, we need to split the dataset and not inspect it, the code used is hidden below.","metadata":{}},{"cell_type":"code","source":"import datetime, pytz\n#define a conversion function for the native timestamps in the csv file\ndef dateparse (time_in_secs):    \n    return pytz.utc.localize(datetime.datetime.fromtimestamp(float(time_in_secs)))\n\n# Data Periods used in Notebook\nplot_period = slice('2020-7-7 0:00','2020-7-7 8:00') # Selectio Plot Period for visualisation only\ndata_period = slice('2020-7-6 13:21','2020-9-14 0:00') # Select Data Period for Analysis\n\n# Path to CSV\npath = '/kaggle/input/bitcoin-historical-data/bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv'","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:17:58.426951Z","iopub.execute_input":"2023-04-27T20:17:58.427519Z","iopub.status.idle":"2023-04-27T20:17:58.446255Z","shell.execute_reply.started":"2023-04-27T20:17:58.427448Z","shell.execute_reply":"2023-04-27T20:17:58.444715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# path = 'bitstampUSD_1-min_data_2012-01-01_to_2020-09-14.csv'\ndf = pd.read_csv(path,parse_dates=[0],\n                 date_parser=dateparse,\n                 index_col='Timestamp')\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:17:58.449902Z","iopub.execute_input":"2023-04-27T20:17:58.450345Z","iopub.status.idle":"2023-04-27T20:18:16.808643Z","shell.execute_reply.started":"2023-04-27T20:17:58.450307Z","shell.execute_reply":"2023-04-27T20:18:16.807246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It's possible to reduce the dataframe memory by 62% (if you need)\n# red_df = red_mem(df)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:16.810411Z","iopub.execute_input":"2023-04-27T20:18:16.811065Z","iopub.status.idle":"2023-04-27T20:18:16.816398Z","shell.execute_reply.started":"2023-04-27T20:18:16.811023Z","shell.execute_reply":"2023-04-27T20:18:16.814776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#CDE10F'>WORKING WITH A SUBSET OF DATA</span></b></p></div>\n\n- Due to the excess ammount of index data available to us, training of models can become quite long, especialy when it comes to cross validaiton.\n- Let's limit the dataset to <code>100,000</code> (compared to 4.5M) recent data points in this notebook, which are assumed to be the most relevant to what we want to predict (future events)\n- A <code>training data</code> period of only two months may not be sufficient to obtain a very accurate model (it's a short period of time, but contains quite a lot of data, nevertheless some trends may be missed), so you could try using even more points, if you have a more powerful PC.\n- Some interesting applications to deal with larger datasets are of course <code>GPU</code> capable models which you could try.\n- A very simple and easy to graps to use <code>XGBoost</code>'s GPU capable model example is provided by [hamditarek](https://www.kaggle.com/hamditarek/market-prediction-xgboost-with-gpu-fit-in-1min?q=XGboost+GPU).","metadata":{}},{"cell_type":"code","source":"df_tr,df_te = TimeSeries_Split(df,split_id=[0.2,None], #  Train/Test Split (0.8/0.2)\n                               cut_id=data_period)     # Use only 100,000 data points ","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:16.820388Z","iopub.execute_input":"2023-04-27T20:18:16.82075Z","iopub.status.idle":"2023-04-27T20:18:16.915738Z","shell.execute_reply.started":"2023-04-27T20:18:16.820709Z","shell.execute_reply":"2023-04-27T20:18:16.914443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#CDE10F'>CLEANING DATA</span></b></p></div>\n\n- The missing data is visualised and is shown to be consisten amongst features.\n- Fill the data of indices during which there were no trades occuring, single-event. <code>value=0</code> (<b>Ignored</b>)\n- Modify features <code>open</code>,<code>high</code>,<code>low</code>,<code>close</code> as the time series is continuous, using forward filling, <code>ffil</code>","metadata":{}},{"cell_type":"code","source":"NaN_values = (df_tr.isnull().sum()/len(df_tr)*100).sort_values(ascending = False)\n\n# Plot missing data\nbar_plot(x = NaN_values,y = NaN_values.index,palette_len = NaN_values.index, \n         xlim = (0,100),xticklabels = range(0,101,20),yticklabels = NaN_values.index,\n         figsize = (10,5), axis_grid = 'x')","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:16.917378Z","iopub.execute_input":"2023-04-27T20:18:16.917712Z","iopub.status.idle":"2023-04-27T20:18:17.201842Z","shell.execute_reply.started":"2023-04-27T20:18:16.91768Z","shell.execute_reply":"2023-04-27T20:18:17.200535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tr[df_tr.isna().any(axis=1)]","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:17.203928Z","iopub.execute_input":"2023-04-27T20:18:17.204445Z","iopub.status.idle":"2023-04-27T20:18:17.22644Z","shell.execute_reply.started":"2023-04-27T20:18:17.204393Z","shell.execute_reply":"2023-04-27T20:18:17.225112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prediction with non events <code>.fillna(0)</code> can be interesting to include in signal modelling, but excluded here to have a more visible stock fluctuation history.","metadata":{}},{"cell_type":"code","source":"def forward_fill_na(ldf):\n    # ldf['Volume_(BTC)'].fillna(0, inplace=True)\n    # ldf['Volume_(Currency)'].fillna(0, inplace=True)\n    # ldf['Weighted_Price'].fillna(0, inplace=True)\n    ldf['Open'].fillna(method='ffill', inplace=True)\n    ldf['High'].fillna(method='ffill', inplace=True)\n    ldf['Low'].fillna(method='ffill', inplace=True)\n    ldf['Close'].fillna(method='ffill', inplace=True)\n    \nforward_fill_na(df_tr)  # modification of training set\nforward_fill_na(df_te) # modification of test set","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:17.227943Z","iopub.execute_input":"2023-04-27T20:18:17.228325Z","iopub.status.idle":"2023-04-27T20:18:17.239426Z","shell.execute_reply.started":"2023-04-27T20:18:17.22829Z","shell.execute_reply":"2023-04-27T20:18:17.238398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df = df.drop_duplicates(keep=False,inplace=True) \ndf_tr = df_tr.dropna() \ndf_te = df_te.dropna()     # replicate on test set","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:17.241171Z","iopub.execute_input":"2023-04-27T20:18:17.241585Z","iopub.status.idle":"2023-04-27T20:18:17.256796Z","shell.execute_reply.started":"2023-04-27T20:18:17.241546Z","shell.execute_reply":"2023-04-27T20:18:17.255479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#CDE10F\"><b><span style='color:#FFFFFF'>3 |</span></b> <b>EXPLORING OUR DATA</b></div>\n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#CDE10F'>DESCRIPTIVE STATISTICS</span></b></p></div>\n\nLet's investigate the statistics of the numerical values of the dataset","metadata":{}},{"cell_type":"code","source":"set_option('precision',2)\ndf_tr.describe()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:17.258702Z","iopub.execute_input":"2023-04-27T20:18:17.259496Z","iopub.status.idle":"2023-04-27T20:18:17.313065Z","shell.execute_reply.started":"2023-04-27T20:18:17.259457Z","shell.execute_reply":"2023-04-27T20:18:17.31187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#CDE10F'>TARGET VARIABLE</span></b></p></div>\n\n- We need to define our prediction variable <code>signal</code>, which will be done via <code>.rolling</code> & <code>.mean()</code> using the <code>Close</code> feature.\n- A short term (window) moving average, <code>SMA1</code> & a long term (window) moving average, <code>SMA2</code> are used to create the target variable, <code>signal</code>.\n- The trading stratergy is as follows; where the <code>Short Term (SMA1)</code> > <code>Long Term (SMA2)</code>, the signal value = 1 <code>(buy)</code>, otherwise it is set to 0 <code>(sell)</code>.\n- The <code>Short Term (SMA1)</code> & <code>Long Term (SMA2)</code> Moving Average value are set to <b>window values of 10 and 60</b> respectively, both of which are arbitrary, and can affect the results, ideally an optimisation study needs to be carried out to find optimum values.","metadata":{}},{"cell_type":"code","source":"def create_target(ldf,tr_id=False):\n    ldf['SMA1'] = ldf['Close'].rolling(window=10, min_periods=1, center=False).mean() #  short simple moving average window\n    ldf['SMA2'] = ldf['Close'].rolling(window=60, min_periods=1, center=False).mean() #  long simple moving average window\n    ldf['signal'] = np.where(ldf['SMA1'] > ldf['SMA2'], 1.0, 0.0) # Create signals\n    if(tr_id is not True):\n        display(ldf['signal'].value_counts())\n    \ndf_tr1 = df_tr.copy()  # Save the Baseline Model Dataframe [Training Set]\ndf_te1 = df_te.copy() # Save the Baseline Model Dataframe [Test Set]\ncreate_target(df_tr1)  # Add target variable to Training Set \ncreate_target(df_te1,tr_id=True)  # Add target variable to Test Set","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:17.314913Z","iopub.execute_input":"2023-04-27T20:18:17.315575Z","iopub.status.idle":"2023-04-27T20:18:17.550347Z","shell.execute_reply.started":"2023-04-27T20:18:17.315536Z","shell.execute_reply":"2023-04-27T20:18:17.549084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We have a relatively even (buy/sell) (40k/38k) target variable <code>signal</code> distribution\n- We don't really have to emphasise issues associated with <code>class imbalance</code> in this problem\n- Simple metrics like <code>accuracy,recall,precision</code> might suffice as <code>classification</code> metrics, instead of detailed ROC & PR curves of classifier models","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#CDE10F'>DATASET TIMESERIES VISUALISATION</span></b></p></div>\n\nLet's visualise the overall asset price history during the <code>training data</code> period & the associated <code>signal</code> as well.","metadata":{}},{"cell_type":"code","source":"plot_vsubplots(df_tr1,['Close','signal'],\n               title='Weighted Price & Signal Fluctional in Training Data',\n               nplots=2,\n               lw_id=[2,0.4],\n               size=[500,1000])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:17.55234Z","iopub.execute_input":"2023-04-27T20:18:17.552724Z","iopub.status.idle":"2023-04-27T20:18:21.475266Z","shell.execute_reply.started":"2023-04-27T20:18:17.552685Z","shell.execute_reply":"2023-04-27T20:18:21.473215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- It's interesting to note the general upward trend of the <code>weighted</code> asset price, having gone up from 9.2k at the start of this period and reaching 11.73k only within a span of a couple of months\n- Not quite easy to visualise it, but the signal (what we will be modelling) is also plotted , and we can observe how much the shorter and longer period MA interchange in this short time period alone","metadata":{}},{"cell_type":"code","source":"# Visualise Training Set Target Variable Related Features\n\nlst_MAV = ['SMA1','SMA2','signal']\nldf = df_tr1.loc[plot_period,lst_MAV]\nplot_line(ldf,lst_MAV,\n          title='SM1, SMA2 & Signal created from Closing Price',\n          sec_id=[False,False,True])  ","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:21.477522Z","iopub.execute_input":"2023-04-27T20:18:21.47863Z","iopub.status.idle":"2023-04-27T20:18:21.586942Z","shell.execute_reply.started":"2023-04-27T20:18:21.478527Z","shell.execute_reply":"2023-04-27T20:18:21.585677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are quite a number of periods during which the shorter and longer moving average values interchange, even for only an <b>8 hour</b> period, during which the cost varied in the range of <code>9240:9400</code> during the observed period, which is indicative of a highly volatile asset.","metadata":{}},{"cell_type":"code","source":"df_tr1=df_tr1.drop(['SMA1','SMA2'], axis=1)\ndf_te1=df_te1.drop(['SMA1','SMA2'], axis=1)   # replicate on test data","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:21.593352Z","iopub.execute_input":"2023-04-27T20:18:21.594068Z","iopub.status.idle":"2023-04-27T20:18:21.607655Z","shell.execute_reply.started":"2023-04-27T20:18:21.594026Z","shell.execute_reply":"2023-04-27T20:18:21.606147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#CDE10F'> 3.1 |</span> Baseline Model Features</b> \n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#CDE10F'>BASELINE FEATURE CORRELATION</span></b></p></div>\n\nLet's define the term baseline; the feature that are available to use in the dataset ie. **open**, **close** etc\n- The linear correlation values of our current features <code>open</code>,<code>high</code>,<code>low</code>,<code>close</code>,<code>volumes</code>,<code>weighted_price</code> to the target variable is very minimal\n- which could suggest a number of things; <code>high nonlinearity</code>, <code>stable oscillation relative to stationary value</code> (circular scatter) or perhaps they are not the most ideal to model the target, <code>signal</code>, and can be improved, so attention shifts to <code>feature engineering</code>","metadata":{}},{"cell_type":"code","source":"corrMat(df_tr1,'signal',figsize=(7,0.5)) # Baseline Dataframe feature correlation to Signal","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:21.609667Z","iopub.execute_input":"2023-04-27T20:18:21.610048Z","iopub.status.idle":"2023-04-27T20:18:21.873266Z","shell.execute_reply.started":"2023-04-27T20:18:21.610011Z","shell.execute_reply":"2023-04-27T20:18:21.872007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#CDE10F'>FEATURE ENGINEERING</span></b></p></div>\n\n- As indicated in the introduction, in the current problem, we will focus on <code>technical indicators</code> as part of our <code>feature engineering</code> approach in an attempt to introduce more relevant features into the <code>feature matrix</code>.\n- It's interesting to know (in the context of a <code>digital asset</code> ), which features have impact on the model's performance, if any.\n\n<b>Specifically:</b>\n\n> - <code>Moving Average</code> : A moving average provides an indication of the trend of the price movement by reducing the amount of noise. <br>\n> - <code>Stochastic Oscillator %K and %D</code> : A stochastic oscillator is a momentum indicator comparing a particular closing price of a security to a range of its prices over a certain period of time. %K and %D are slow and fast indicators. <br>\n> - <code>Relative Strength Index(RSI)</code> : It is a momentum indicator that measures the magnitude of recent price changes to evaluate overbought or oversold conditions in the price of a stock or other asset. Ranging from [0,100]. <b>Asset -> 70: asset deemed overbought</b>. <b>Asset -> 30: asset getting undersold & undervalued.</b><br>\n> - <code>Rate Of Change(ROC)</code>: It is a momentum oscillator, which measures the percentage change between the current price and the n period past price. Assets with <b>higher ROC values</b> are considered more likely to be overbought & <b>lower ROC</b>; more likely to be oversold.<br>\n> - <code>Momentum (MOM)</code> : It is the rate of acceleration of a security's price or volume; the speed at which the price is changing. <br>\n\nCan be all be potentially useful to model the target variable, <code>signal</code>, with of course varing degress of influence.","metadata":{}},{"cell_type":"code","source":"df_tr2 = df_tr1.copy()  # Create duplicate dataframe & add features to it\ndf_te2 = df_tr2.copy()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:21.874671Z","iopub.execute_input":"2023-04-27T20:18:21.875117Z","iopub.status.idle":"2023-04-27T20:18:21.882344Z","shell.execute_reply.started":"2023-04-27T20:18:21.87508Z","shell.execute_reply":"2023-04-27T20:18:21.881212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' Technical Indicators '''\n\n#Calculation of moving average\ndef ma(df, n):\n    return pd.Series(df['Close'].rolling(n, min_periods=n).mean(), name='MA_' + str(n))\n\n# exponentially weighted moving average \ndef ema(df, n):\n    return pd.Series(df['Close'].ewm(span=n,min_periods=n).mean(), name='EMA_' + str(n))\n\n#Calculation of price momentum\ndef mom(df, n):     \n    return pd.Series(df.diff(n), name='Momentum_' + str(n))  \n\n# rate of change\ndef roc(df, n):  \n    M = df.diff(n - 1) ; N = df.shift(n - 1)  \n    return pd.Series(((M / N) * 100), name = 'ROC_' + str(n)) \n\n# relative strength index\ndef rsi(df, period):\n    delta = df.diff().dropna()\n    u = delta * 0; d = u.copy()\n    u[delta > 0] = delta[delta > 0]; d[delta < 0] = -delta[delta < 0]\n    u[u.index[period-1]] = np.mean( u[:period] ) #first value is sum of avg gains\n    u = u.drop(u.index[:(period-1)])\n    d[d.index[period-1]] = np.mean( d[:period] ) #first value is sum of avg losses\n    d = d.drop(d.index[:(period-1)])\n    rs = u.ewm(com=period-1, adjust=False).mean() / d.ewm(com=period-1, adjust=False).mean()\n    return 100 - 100 / (1 + rs)\n\n# stochastic oscillators slow & fast\ndef sto(close, low, high, n,id): \n    stok = ((close - low.rolling(n).min()) / (high.rolling(n).max() - low.rolling(n).min())) * 100\n    if(id is 0):\n        return stok\n    else:\n        return stok.rolling(3).mean()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:21.883768Z","iopub.execute_input":"2023-04-27T20:18:21.884346Z","iopub.status.idle":"2023-04-27T20:18:21.901329Z","shell.execute_reply.started":"2023-04-27T20:18:21.884307Z","shell.execute_reply":"2023-04-27T20:18:21.900011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tech_indi(ldf,tr_id=True):\n\n    ''' Moving Average '''\n    ldf['MA21'] = ma(ldf,10)\n    ldf['MA63'] = ma(ldf, 30)\n    ldf['MA252'] = ma(ldf, 200)\n    lst_MA = ['MA21','MA63','MA252']\n\n    ''' Exponentially Weighted Moving Average '''\n    ldf['EMA10'] = ema(ldf, 10)\n    ldf['EMA30'] = ema(ldf, 30)\n    ldf['EMA200'] = ema(ldf, 200)\n    lst_EMA = ['EMA10','EMA30','EMA200']\n\n    ''' Momentum '''\n    ldf['MOM10'] = mom(ldf['Close'], 10)\n    ldf['MOM30'] = mom(ldf['Close'], 30)\n    lst_MOM = ['MOM10','MOM30']\n\n    ''' Relative Strength Index '''\n    ldf['RSI10'] = rsi(ldf['Close'], 10)\n    ldf['RSI30'] = rsi(ldf['Close'], 30)\n    ldf['RSI200'] = rsi(ldf['Close'], 200)\n    lst_RSI = ['RSI10','RSI30','RSI200']\n\n    ''' Slow Stochastic Oscillators '''\n    ldf['%K10'] = sto(ldf['Close'], ldf['Low'], ldf['High'],5,0)\n    ldf['%K30'] = sto(ldf['Close'], ldf['Low'], ldf['High'],10,0)\n    ldf['%K200'] = sto(ldf['Close'], ldf['Low'], ldf['High'], 20,0)\n    lst_pK = ['%K10','%K30','%K200']\n\n    ''' Fast Stochastic Oscillators '''\n    ldf['%D10'] = sto(ldf['Close'], ldf['Low'], ldf['High'], 10,1)\n    ldf['%D30'] = sto(ldf['Close'], ldf['Low'], ldf['High'], 30,1)\n    ldf['%D200'] = sto(ldf['Close'], ldf['Low'], ldf['High'], 200,1)\n    lst_pD = ['%D10','%D30','%D200']\n    \n    # Plot Training Data\n    if(tr_id):\n        plot_line(ldf.loc[plot_period,lst_MA],lst_MA,title='Moving Average (window=21,63,252)')\n        plot_line(ldf.loc[plot_period,lst_EMA],lst_EMA,title='Exponential Moving Average (window=10,30,200)')\n        plot_line(ldf.loc[plot_period,lst_MOM],lst_MOM,title='Momentum')\n        plot_line(ldf.loc[plot_period,lst_RSI],lst_RSI,title='Relative Strength Index')\n        plot_line(ldf.loc[plot_period,lst_pK],lst_pK,title='Stochastic Oscillators (slow)')\n        plot_line(ldf.loc[plot_period,lst_pD],lst_pD,title='Stochastic Oscillators (Fast)')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-27T20:18:21.903067Z","iopub.execute_input":"2023-04-27T20:18:21.903892Z","iopub.status.idle":"2023-04-27T20:18:21.920846Z","shell.execute_reply.started":"2023-04-27T20:18:21.903841Z","shell.execute_reply":"2023-04-27T20:18:21.919832Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tech_indi(df_tr2) # add technical features to training set\ntech_indi(df_te2,tr_id=False) # add technical features to test set","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:21.92224Z","iopub.execute_input":"2023-04-27T20:18:21.922781Z","iopub.status.idle":"2023-04-27T20:18:22.743264Z","shell.execute_reply.started":"2023-04-27T20:18:21.922717Z","shell.execute_reply":"2023-04-27T20:18:22.74223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# All the current features\ndf_tr2.columns","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:22.746182Z","iopub.execute_input":"2023-04-27T20:18:22.746952Z","iopub.status.idle":"2023-04-27T20:18:22.754498Z","shell.execute_reply.started":"2023-04-27T20:18:22.746899Z","shell.execute_reply":"2023-04-27T20:18:22.753167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#CDE10F'> 3.2 |</span> Updated Feature Model Features</b> \n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#CDE10F'>UPDATED FEATURE LINEAR CORRELATION</span></b></p></div>\n\n- Having created new features; <code>MA</code>,<code>EMA</code>,<code>MOM</code>,<code>RSI</code>,<code>%K/%D</code>,\n- let's investigate the linear correlation of these new featuers to the target variable & compare to the <code>baseline dataset</code> features","metadata":{}},{"cell_type":"code","source":"corrMat(df_tr2,'signal',figsize=(15,0.5))","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:22.755824Z","iopub.execute_input":"2023-04-27T20:18:22.756777Z","iopub.status.idle":"2023-04-27T20:18:23.40066Z","shell.execute_reply.started":"2023-04-27T20:18:22.756731Z","shell.execute_reply":"2023-04-27T20:18:23.399415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see the significanly more linearly correlated group of features that were created as a result of <code>feature engineering</code>\n- It's likely that the <code>base</code> dataset features will have little to no impact on target variable variation if used in the <code>feature matrix</code>\n- On the otherhand, the newly created features have a reasonably wide range of correlated values, and quite important; are not too highly correlated to the target variable, <code>signal</code>","metadata":{}},{"cell_type":"code","source":"def drp_feat(ldf):\n    ldf = ldf.drop(['High','Low','Open','Volume_(Currency)'], axis=1) # let's drop most of the original feature\n    \ndrp_feat(df_tr2)\ndrp_feat(df_te2)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:23.402321Z","iopub.execute_input":"2023-04-27T20:18:23.403339Z","iopub.status.idle":"2023-04-27T20:18:23.431081Z","shell.execute_reply.started":"2023-04-27T20:18:23.403296Z","shell.execute_reply":"2023-04-27T20:18:23.429817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Having applied functions to our <code>feature matrix</code>, we need to recheck for missing data.","metadata":{}},{"cell_type":"code","source":"NaN_values = (df_tr2.isnull().sum() / len(df_tr2) * 100).sort_values(ascending = False)\nbar_plot(x = NaN_values,y = NaN_values.index,palette_len = NaN_values.index, \n         xlim = (0,1),xticklabels = range(0,10),yticklabels = NaN_values.index,\n         figsize = (10,5), axis_grid = 'x')","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:23.433305Z","iopub.execute_input":"2023-04-27T20:18:23.433993Z","iopub.status.idle":"2023-04-27T20:18:23.852443Z","shell.execute_reply.started":"2023-04-27T20:18:23.433952Z","shell.execute_reply":"2023-04-27T20:18:23.851242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tr2 = df_tr2.dropna() \ndf_te2 = df_te2.dropna()\ndf_tr2.info()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:23.853779Z","iopub.execute_input":"2023-04-27T20:18:23.854124Z","iopub.status.idle":"2023-04-27T20:18:23.889387Z","shell.execute_reply.started":"2023-04-27T20:18:23.85409Z","shell.execute_reply":"2023-04-27T20:18:23.888222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#CDE10F\"><b><span style='color:#FFFFFF'>4 |</span></b> <b>MODEL GENERATION</b></div>\n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#CDE10F'>EVALUATION FUNCTION</span></b></p></div>\n\n\nHaving defined a clear <code>target variable</code> & <code>feature matrix</code>, let's review what we have: <br><br>\n- df_tr1/df_te1 : <code>Training/Test dataframe of baseline features associated to the asset</code>\n- df_tr2/df_te2 : <code>Training/Test dataframe of newly created features created in feature engineering stage </code>\n\nAnd we can start making models to predict the target variable <code>signal</code> (market directivity), using the evaluation function below.\n\n<b>The Evaluation Function is (hidden below):</b><br><br>\nThe aim of the evaluation function is to evaluate how well the model performs on different data split & evaluation approaches.\n\n<b>(1)</b> The function takes in a <code>dataframe</code> which contains both the <code>feature matrix, X</code> & <code>target variable, y</code>. <br>\n<b>(2)</b> The data is split into two parts; <code>train_df</code> & <code>eval_df</code> <br>\n<b>(3)</b> A 5-Fold <code>cross validation</code> evaluation of the imported dataframe is evaluated to get a picture of how well the model performs on the training data (both little and big chunks)<br>\n<b>(4)</b> A standard Two-Way Split (without data shuffling) is made, and trained on <code>X_train/y_train</code> & <code>X_eval/y_eval</code>","metadata":{}},{"cell_type":"code","source":"models = []\n# Lightweight Models \nmodels.append(('LDA', LinearDiscriminantAnalysis()))  # Unsupervised Model \nmodels.append(('KNN', KNeighborsClassifier()))  # Unsupervised Model\nmodels.append(('TREE', DecisionTreeClassifier())) # Supervised Model\nmodels.append(('NB', GaussianNB())) # Unsupervised Model\n\n# More Advanced Models\nmodels.append(('GBM', GradientBoostingClassifier(n_estimators=25)))\nmodels.append(('XGB',XGBClassifier(n_estimators=25,eval_metric='logloss')))\nmodels.append(('CAT',CatBoostClassifier(silent=True,\n                                        n_estimators=25)))\nmodels.append(('RF', RandomForestClassifier(n_estimators=25)))","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:23.890904Z","iopub.execute_input":"2023-04-27T20:18:23.891292Z","iopub.status.idle":"2023-04-27T20:18:23.90109Z","shell.execute_reply.started":"2023-04-27T20:18:23.891256Z","shell.execute_reply":"2023-04-27T20:18:23.900212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# eval_id (T/F): [CV,Train,Test,all]\n\ndef modelEval(ldf,feature='signal',split_id=[None,None],eval_id=[True,True,True,True],\n              n_fold=5,scoring='accuracy',plot_id=[False,True],cv_yrange=None,hm_vvals=[0.5,1.0,0.75]):\n    \n    print('Evaluation Function')\n    print(f'Cross Validation Activated, n_splits : {n_fold}, scoring metric: {scoring}')\n    if(eval_id[2]):\n        if(split_id[0] is not None):\n            print(f'Train/Evaluation Set Spit Activated: {split_id[0]}')\n        if(split_id[1] is not None):\n            print(f'Train/Evaluation Set Split made at {split_id[1]}')\n    \n    ''' 1. Split Train/Evaluation <DataFrame> Set Split '''\n    \n    # split_id : Train/Test split [%,timestamp], whichever is not None\n    # test_id : Evaluate trained model on test set only\n    \n    if(split_id[0] is not None):\n        # General Percentage Split (Non Shuffle requied for Time Series)\n        train_df,eval_df = train_test_split(ldf,test_size=split_id[0],shuffle=False)\n    elif(split_id[1] is not None):\n        # specific time split \n        train_df = df.loc[:split_id[1]]; eval_df = df.loc[split_id[1]:] \n    else:\n        print('Choose One Splitting Method Only')\n        \n    ''' 2. Train/Test Feature Matrices + Target Variables Split'''\n    \n    y_train = train_df[feature]\n    X_train = train_df.loc[:, train_df.columns != feature]\n    y_eval = eval_df[feature]\n    X_eval = eval_df.loc[:, eval_df.columns != feature]\n    X_one = pd.concat([X_train,X_eval],axis=0)\n    y_one = pd.concat([y_train,y_eval],axis=0)\n    \n    print('');print(f'Using Features: {X_train.columns}')\n    print(f'Target Variable: {feature}');print('')\n        \n    ''' 3. Visualise Training/Test Data'''\n    if(plot_id[0]):\n        \n        # plot the training data\n        fig = go.Figure()\n        fig.add_trace(go.Scatter(x=train_df.index, y=train_df['signal'],mode='lines',name='Training Data', line={'width': 0.25}))\n        fig.update_layout(height=300,width=800,template='plotly_white',title='Training Signal Visualisation',\n                          margin=dict(l=50,r=80,t=50,b=40))\n        \n        # Plot the test data as well \n        if(eval_id[2]):\n            fig.add_trace(go.Scatter(x=eval_df.index, y=eval_df['signal'],mode='lines',name='Test Data',line={'width': 0.25}))\n            fig.update_layout(title='Training/Test Signal Visualisation')\n        fig.show()\n    \n    ''' 4. Cross Validation, Training/Evaluation, one evaluation'''\n    lst_res = []; names = []; lst_train = []; lst_eval = []; lst_one = []; lst_res_mean = []\n    if(any(eval_id)):\n        for name, model in models:  # cycle through models & evaluate either cv or train/test\n            names.append(name)\n            \n            # Cross Validation Model on Training Se\n            if(eval_id[0]):\n                t0=time.time()\n                kfold = KFold(n_splits=n_fold)\n                cv_res = cross_val_score(model,X_train,y_train, cv=kfold, scoring=scoring)\n                t1 = time.time()\n                lst_res.append(cv_res)\n                tt1 = t1-t0 # total time for n_fold cross evaluation\n                \n            # Evaluate Fit Model on Training Data\n            t2 = time.time()\n            if(eval_id[1]):\n                t2 = time.time()\n                res = model.fit(X_train,y_train)\n                train_res = accuracy_score(res.predict(X_train),y_train); lst_train.append(train_res)\n            if(eval_id[2]):\n                if(eval_id[1] is False):  # If training hasn't been called yet\n                    res = model.fit(X_train,y_train)\n                eval_res = accuracy_score(res.predict(X_eval),y_eval); lst_eval.append(eval_res)\n            t3 = time.time()\n            tt2 = t3-t2 # total time for training/evaluation train/prediction\n            \n            # Evaluate model on entire dataset\n            if(eval_id[3]):\n                t4 = time.time()\n                res = model.fit(X_one,y_one)\n                one_res = accuracy_score(res.predict(X_one),y_one); lst_one.append(one_res)\n                t5 = time.time()\n                tt3 = t5-t4 # total time for training & evaluation on whole dataframe\n            \n            ''' [out] Verbal Outputs '''\n            # Cross Validation / Training / Evaluation Model Evaluation / Section Times\n            lst_res_mean.append(cv_res.mean())\n            fn1 = cv_res.mean(); fn2 = cv_res.std();\n            fn3 = train_res; fn4 = eval_res; fn5 = one_res\n            print(f\"{name} : {fn1:.3f}({fn2:.3f}) -> {tt1:.2f}s | {fn3:.3f} & {fn4:.3f} -> {tt2:.2f}s | {fn5:.3f} -> {tt3:.2}s\")\n      \n    s0 = pd.Series(np.array(lst_res_mean),index=names)\n    s1 = pd.Series(np.array(lst_train),index=names)\n    s2 = pd.Series(np.array(lst_eval),index=names)\n    s3 = pd.Series(np.array(lst_one),index=names)\n    pdf = pd.concat([s0,s1,s2,s3],axis=1)\n    pdf.columns = ['cv_average','train','test','all']\n    s4 = pd.Series([tt1,tt2,tt3],index=['cv','train/test','all'])\n        \n    ''' 5. Visual Ouputs '''\n    if(plot_id[1]): \n        \n        sns.set(style=\"whitegrid\")\n        fig,ax = plt.subplots(1,2,figsize=(15,4))\n        ax[0].set_title(f'{n_fold} Cross Validation Results')\n        sns.boxplot(data=lst_res, ax=ax[0], orient=\"v\",width=0.3)\n        ax[0].set_xticklabels(names)\n        sns.stripplot(data=lst_res,ax=ax[0], orient='v',color=\".3\",linewidth=1)\n        ax[0].set_xticklabels(names)\n        ax[0].xaxis.grid(True)\n        ax[0].set(xlabel=\"\")\n        if(cv_yrange is not None):\n            ax[0].set_ylim(cv_yrange)\n        sns.despine(trim=True, left=True)\n    \n        sns.heatmap(pdf,vmin=hm_vvals[0],vmax=hm_vvals[1],center=hm_vvals[2],\n                    ax=ax[1],square=False,lw=2,annot=True,fmt='.3f',cmap='Blues')\n        ax[1].set_title('Accuracy Scores')\n        plt.show()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-04-27T20:18:23.902731Z","iopub.execute_input":"2023-04-27T20:18:23.903061Z","iopub.status.idle":"2023-04-27T20:18:23.935445Z","shell.execute_reply.started":"2023-04-27T20:18:23.903028Z","shell.execute_reply":"2023-04-27T20:18:23.934112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#CDE10F'> 4.1 |</span> BASELINE FEATURE MODEL EVALUATION</b> \n\n- The **<span style='color:#CDE10F'>Baseline Features</span>** include:\n> The asset's minute's <code>open</code>,<code>high</code>,<code>low</code>,<code>close</code>,<code>Volume_(BTC)</code>,<code>Volume_(Currency)</code> & <code>Weighted_Price</code>\n- As I found out, in time series applications, it's not very common to use base features associated with one asset only, but let's see how it fairs anyway, let's also plot the training data to get a visual idea of what we are attempting to model.","metadata":{}},{"cell_type":"code","source":"modelEval(df_tr1,split_id=[0.2,None],plot_id=[False,True])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:23.937202Z","iopub.execute_input":"2023-04-27T20:18:23.93825Z","iopub.status.idle":"2023-04-27T20:20:06.230817Z","shell.execute_reply.started":"2023-04-27T20:18:23.938196Z","shell.execute_reply":"2023-04-27T20:20:06.229253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see that the **<span style='color:#CDE10F'>cross_val_score</span>** is hovering in the region of **<span style='color:#CDE10F'>accuracy = 0.5</span>**, which suggests that using only **<span style='color:#CDE10F'>baseline features</span>** associated with one asset isn't quite suitable to predict accurate **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">asset directivity</mark>**\n- Most models tended to have a higher **<span style='color:#CDE10F'>training score</span>** than the **<span style='color:#CDE10F'>cross validation score</span>**\n- It was interesting to see that the **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">DecisionTreeClassifier</mark>** & **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">RandomForest</mark>**, even with very few estimators are able to achieve very high scores (be it overfitted)\n- This suggests tree based models could be very useful in this problem & **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">kNN</mark>** can also be added to the list of someone overfitting models on the training data as they tend to have lower cross validation scores\n\n**<span style='color:#CDE10F'>Training & Evaluation time</span>** also is quite important in this problem:\n- Having used only (100k/4.5M), the cost even with 7 features is quite high for the more advanced models, (<code>esp.GBM & ANN</code>); \n- More advanced models had to be tuned down significantly to reduce the training time to comparable levels, therefore it's desirable to optimise the feature selection process.\n- **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">XGB</mark>** & **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">CATBoost</mark>**, surprising were quite quick for quite advanced model, indicating it's quite well optimised for being used right out of the box. \n- **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">RandomForest</mark>** being a similar model to XGB is much slower.","metadata":{}},{"cell_type":"markdown","source":"### <b><span style='color:#CDE10F'> 4.2 |</span> Updated feature model evaluation</b> \n\nWe created new features in the <code>feature engineering</code> in <code>Section 2.5</code>, generating the updated features: <code>df_feat</code> dataframe, let's retry with these new features. ","metadata":{}},{"cell_type":"code","source":"modelEval(df_tr2,\n          split_id=[0.2,None],\n          plot_id=[False,True],\n          cv_yrange=(0.8,1.0),\n          hm_vvals=[0.8,1.0,0.9])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:20:06.232738Z","iopub.execute_input":"2023-04-27T20:20:06.233129Z","iopub.status.idle":"2023-04-27T20:27:53.709014Z","shell.execute_reply.started":"2023-04-27T20:20:06.23309Z","shell.execute_reply":"2023-04-27T20:27:53.707629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see a very significant improvement in the **<span style='color:#CDE10F'>accuracy</span>** scores, when compared to the **<span style='color:#CDE10F'>baseline model</span>**\n- **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">LinearDiscriminantAnalysis()</mark>** performs surprising well, for not only on the training set but also in the cross validation, it's also one of the fastest approaches, making it one of the most efficient approaches for large datasets\n- Among the higher scoring model **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">LDA</mark>**, are not surprisingly more advanced models, **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">GBM</mark>**,**<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">XGB</mark>**,**<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">CAT</mark>**,**<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">RF</mark>** as well\n- **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">kNN()</mark>** and **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">GaussianNB()</mark>** unsupervised models performed slightly worse, in comparison to supervised learning models","metadata":{}},{"cell_type":"markdown","source":"\n# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#CDE10F\"><b><span style='color:#FFFFFF'>5 |</span></b> <b>MODEL EFFICIENCY OPTIMISATION</b></div>\n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#CDE10F'>UTILISING DIMENSIONALITY REDUCTION</span></b></p></div>\n\n\n- A big issue encounted when trying to reach the objective in the problem is the large number of tick data (every minute) in the entire dataset, which increases the computational training & evaluation cost quite a bit.\n- Since the **<span style='color:#CDE10F'>feature matrices</span>** are dependent on the number of features & instances, a reduction of even one unnecessary feature woud have a visible impact on the computational cost.\n- It is therefore of upmost importance to <b>reduce unnecessary features</b> as much as possible, to the problem \nbalancing act between model accuracy and training/prediction speed.\n\n<b>Let's look at two approaches one might take:</b>\n\n- <b>(1) Dimensionality Reduction via Feature Importance Evaluation (which is a more manual process) </b>\n\n> Although this is more of a manual process since all libraries are not combine, let's try to find common ground between all approaches and combine them into one <code>feature importance</code> evaluation approach function to allow us to identify, evaluate and remove features to speed up our approach.\n\n- <b>(2) Dimensionality Reduction using Unsupervised Learning Algorithms (more automated process)</b>\n\n> A rather straightforward <code>fit & transform</code> collection of powerful <code>dimension reduction</code> algorithms are available to us in the <code>sklearn</code> library, the only issue I can think of is that; explaining what the resultant features mean may be a little problematic.","metadata":{}},{"cell_type":"markdown","source":"### <b><span style='color:#CDE10F'> 5.1 |</span> Dimensionality reduction via feature importance</b> \n\n- We can look at the <b>Feature Importance</b> (FI) of certain trained models to understand which features & to what extent.\n- We can use such minimalistic functions to quicky evaluat feature importance by relying on <b>variation of approaches</b> & <b>optimised libraries</b>.\n- We can obtain <b>relative feature importance</b> using different libraries , <b>function</b> <code>feature_importance</code> includes:\n\n> - **<span style='color:#CDE10F'>Linear Correlation</span>** w/ abs() function.\n> - **<span style='color:#CDE10F'>SHAP Values</span>** of Catboost Regression Model (n_est)\n> - **<span style='color:#CDE10F'>RandomForest Regressor</span>** (n_est)\n> - **<span style='color:#CDE10F'>XGBoost Regressor</span>** (n_est)\n> - **<span style='color:#CDE10F'>CatBoost Regressor</span>** (n_est)\n> - **<span style='color:#CDE10F'>SelectKBest</span>** (k)\n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#CDE10F'>ADDITIONAL ADJUSTMENTS</span></b></p></div>\n\n- The indivual scores are combined and scaled using <code>MinMaxScaler()</code> & Plot.\n- The y-axis represents the total score (higher score is better, max -> Number of approaches).\n- The x-axis represents the corresponding features of input dataframe.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest,f_regression\nfrom xgboost import plot_importance,XGBRegressor\nfrom catboost import CatBoostClassifier,CatBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import preprocessing\nimport shap\nimport seaborn as sns\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Plot Correlation to Target Variable only\ndef corrMat(df,target='signal',figsize=(9,0.5),ret_id=False):\n    \n    corr_mat = df.corr().round(2)\n    shape = corr_mat.shape[0]\n    corr_mat = corr_mat.transpose()\n    corr = corr_mat.loc[:, df.columns == target].transpose().copy()\n    \n    if(ret_id):\n        return corr\n\n''' Feature Importance '''\n# Various Approaches for quick FI evaluation\n\ndef fi(ldf,target='signal',n_est=25,num_only=True,\n       drop_id=None,target_cat=True,drop_na=False):\n    \n    # Select only numerical features\n    if(num_only):\n        ldf = ldf.select_dtypes(include=['float64','int64'])    \n    \n    # Drop all NaN\n    if(drop_na):\n        print(f'Before NaN drop: {ldf.shape}')\n        ldf = ldf.dropna()\n        print(f'After NaN dropped: {ldf.shape}')\n    \n    \n    ldf = ldf.copy()\n    # If target is categorical string variable\n    if(target_cat):\n        cats = ldf[target].unique()\n        cats_id = [i for i in range(0,len(cats))]\n        maps = dict(zip(cats,cats_id))    \n        ldf[target] = ldf[target].map(maps)\n    \n    # If any features are desired to be droped \n    if(drop_id is not None):\n        ldf = ldf.drop(drop_id,axis=1)\n\n    # Input dataframe containing feature & target variable\n    y = ldf[target]\n    X = ldf.drop(target,axis=1)\n    \n#   CORRELATION\n    imp = corrMat(ldf,target,figsize=(15,0.5),ret_id=True)\n    del imp[target]\n    s1 = imp.squeeze(axis=0);s1 = abs(s1)\n    s1.name = 'CORR'\n    \n#   SHAP\n    model = CatBoostRegressor(silent=True,n_estimators=n_est).fit(X,y)\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X)\n    shap_sum = np.abs(shap_values).mean(axis=0)\n    s2 = pd.Series(shap_sum,index=X.columns,name='CAT_SHAP').T\n    \n#   CATBOOST\n    model = CatBoostRegressor(silent=True,n_estimators=n_est).fit(X,y)\n    fit = model.fit(X,y)\n    rf_fi = pd.DataFrame(model.feature_importances_,index=X.columns,\n                                         columns=['CAT'])\n    rf_fi.sort_values('CAT',ascending=False)\n    s3 = rf_fi.T.squeeze(axis=0)\n    \n#   RANDOMFOREST\n    model = RandomForestRegressor(n_est,random_state=0, n_jobs=-1)\n    fit = model.fit(X,y)\n    rf_fi = pd.DataFrame(model.feature_importances_,index=X.columns,\n                                         columns=['RF'])\n    rf_fi.sort_values('RF',ascending=False)\n    s4 = rf_fi.T.squeeze(axis=0)\n\n#   XGB \n    model=XGBRegressor(n_estimators=n_est,learning_rate=0.5,verbosity = 0)\n    model.fit(X,y)\n    data = model.feature_importances_\n    s5 = pd.Series(data,index=X.columns,name='XGB').T\n\n#   KBEST\n    model = SelectKBest(k=5, score_func=f_regression)\n    fit = model.fit(X,y)\n    data = fit.scores_\n    s6 = pd.Series(data,index=X.columns,name='KBEST')\n\n    # Combine Scores\n    df0 = pd.concat([s1,s2,s3,s4,s5,s6],axis=1)\n    df0.rename(columns={'target':'lin corr'})\n\n    # MinMax Scaler\n    x = df0.values \n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(x)\n    df = pd.DataFrame(x_scaled,index=df0.index,columns=df0.columns)\n    df = df.rename_axis(f'<b>FI APPROACH</b>', axis=1)\n    df = df.rename_axis('Feature', axis=0)\n    \n    pd.options.plotting.backend = \"plotly\"\n    fig = df.plot(kind='bar',title='<b>SCALED FEATURE IMPORTANCE</b>',\n                  color_discrete_sequence=px.colors.qualitative.T10)\n    fig.update_layout(template='plotly_white',height=400,\n                     font=dict(family='sans-serif',size=12),\n                     margin=dict(l=60, r=40, t=50, b=10))\n    fig.update_traces(width=0.25)\n    fig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-27T20:27:53.711851Z","iopub.execute_input":"2023-04-27T20:27:53.712243Z","iopub.status.idle":"2023-04-27T20:27:53.746595Z","shell.execute_reply.started":"2023-04-27T20:27:53.712206Z","shell.execute_reply":"2023-04-27T20:27:53.744876Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fi(df_tr2)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:27:53.748307Z","iopub.execute_input":"2023-04-27T20:27:53.748693Z","iopub.status.idle":"2023-04-27T20:28:22.308436Z","shell.execute_reply.started":"2023-04-27T20:27:53.748656Z","shell.execute_reply":"2023-04-27T20:28:22.307151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can note that for a lot of features, a small value in <code>correlation</code> magnitude (Pearson's value) also gives small score values in other approaches. Similarly high <code>correlated</code> features tend to have high scores in other <code>feature importance</code> methods, which is quite interesting. There are some exceptions, like <code>%D10</code> & <code>MOM10</code>\n- Whilst there are a few features which show some slight dissagreement when it comes to feature importance, overall, <b>feature score similarity can be observed  for most approaches</b>\n- It's interesting to note scores of identical feature cases (eg. <code>MOM10</code>, <code>MOM30</code>); we can get an idea of potentially new features that could we could try out ( perhaps <code>MOM20</code> would have worked better than <code>MOM10</code> )\n- We can observe a lot of features that have a <b>very low relative score value</b> for most methods, and hence probably have little to no impact, even if they were to be removed\n- Removing potentially unimpactful features (which is around 50% of them) would make our whole approach much more efficient, and allow us to focus on more lengthy and in depth <code>hyperparameter</code> gridsearches that hopefully will be more accurate than any of our current models","metadata":{}},{"cell_type":"code","source":"df_tr2_FI = df_tr2.drop(columns=['Open','High','Low','Close','Volume_(BTC)','Volume_(Currency)','Weighted_Price','MA63','EMA10','%K10'])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:28:22.309717Z","iopub.execute_input":"2023-04-27T20:28:22.310047Z","iopub.status.idle":"2023-04-27T20:28:22.318961Z","shell.execute_reply.started":"2023-04-27T20:28:22.310015Z","shell.execute_reply":"2023-04-27T20:28:22.317492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelEval(df_tr2_FI,split_id=[0.2,None],plot_id=[False,True],cv_yrange=(0.8,1.0),hm_vvals=[0.8,1.0,0.9])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:28:22.320401Z","iopub.execute_input":"2023-04-27T20:28:22.320965Z","iopub.status.idle":"2023-04-27T20:30:40.698934Z","shell.execute_reply.started":"2023-04-27T20:28:22.320927Z","shell.execute_reply":"2023-04-27T20:30:40.697551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#CDE10F'> 5.2 |</span> Dimensionality reduction using unsupervised learning algorithms</b> \n\nAn alternative approach to **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">feature reduction</mark>** is the utilisation of **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">Unsupervised Learning</mark>** methods.\n- We need to select an algorithm (it's best to look at quite a few and see how they perform),\n- Perhaps apply some **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">scaling</mark>** & simply <code>fit_transform</code> to get the modified <code>feature matrix</code> which will have the selected dimension\n\nThe next function contains the following selectable **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">Unsupervised Learning</mark>** algorithms to achieve dimensionaliy reduction:\n> - **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">PCA</mark>**, **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">Sparse PCA</mark>**, **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">Kernel PCA</mark>**, **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">Incremental PCA</mark>**, **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">Truncated SVD</mark>** \n> - **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">Fast ICA</mark>**, **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">Gaussian Random Projection</mark>**, **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">Sparse Random Projection</mark>**\n> - **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">IsoMap</mark>** (Manifold),**<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">MDS</mark>** (Manifold),**<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">TSNE</mark>** (Manifold)\n> - **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">Locally Linear Embedding</mark>**, **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">Mini Batch Dictionary Learning</mark>**\n\n- Not all of them are realisable on Kaggle due to the limited computational memory (esp. <b>Manifold</b> approaches), even with the <code>red_mem</code> function activated\n- It's not too uncommon to use multi stage approaches for **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">manifold</mark>** methods, eg. a **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">PCA</mark>** step before **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">manifold</mark>** approach\n- Here we'll use approaches that are requite less computational resources to run","metadata":{}},{"cell_type":"code","source":"def dimRed(ldf,feature='signal',split_id=[None,None],n_comp=5,plot_id=True,\n           model_id='sparserandomprojection',scaler_id=[False,None]):\n    \n    # Given a dataframe, split feature/target variable\n    X = ldf.copy()\n    y = ldf[feature].copy()\n    del X[feature]\n    \n    n_jobs = -1; rs = 32\n    \n    if(model_id is 'pca'):\n        whiten = False\n        model = PCA(n_components=n_comp,whiten=whiten,random_state=rs)\n    if(model_id is 'sparsepca'):\n        alpha = 1\n        model = SparsePCA(n_components=n_comp,alpha=alpha,random_state=rs,n_jobs=n_jobs)\n    elif(model_id is 'kernelpca'):\n        kernel = 'rbf'; gamma = None\n        model = KernelPCA(n_components=n_comp,kernel=kernel,gamma=gamma,n_jobs=n_jobs,random_state=rs)\n    elif(model_id is 'incrementalpca'):\n        batch_size = None\n        model = IncrementalPCA(n_components=n_comp,batch_size=batch_size)\n    elif(model_id is 'truncatedsvd'): \n        algorithm = 'randomized';n_iter = 5\n        model = TruncatedSVD(n_components=n_comp,algorithm=algorithm,n_iter=n_iter,random_state=rs)\n    elif(model_id is 'gaussianrandomprojection'):\n        eps = 0.5\n        model = GaussianRandomProjection(n_components=n_comp,eps=eps,random_state=rs)\n    elif(model_id is 'sparserandomprojection'):\n        density = 'auto'; eps = 0.5; dense_output = True\n        model = SparseRandomProjection(n_components=n_comp,density=density, \n                                       eps=eps, dense_output=dense_output,random_state=rs)\n    if(model_id is 'isomap'):\n        n_neigh = 2\n        model = Isomap(n_neighbors=n_neigh,n_components=n_comp, n_jobs=n_jobs)    \n    elif(model_id is 'mds'):\n        n_init = 1; max_iter = 50; metric = False\n        model = MDS(n_components=n_comp,n_init=n_init,max_iter=max_iter,metric=True,\n                    n_jobs=n_jobs, random_state=rs)\n    elif(model_id is 'locallylinearembedding'):\n        n_neigh = 10; method = 'modified'\n        model = LocallyLinearEmbedding(n_neighbors=n_neigh,n_components=n_comp, method=method, \\\n                                    random_state=rs, n_jobs=n_jobs)\n    elif(model_id is 'tsne'):\n        learning_rate = 300; perplexity = 30; early_exaggeration = 12; init = 'random'\n        model = TSNE(n_components=n_comp, learning_rate=learning_rate, \\\n                    perplexity=perplexity, early_exaggeration=early_exaggeration, \\\n                    init=init, random_state=rs)\n    elif(model_id is 'minibatchdictionarylearning'):\n        alpha = 1; batch_size = 200; n_iter = 25\n        model = MiniBatchDictionaryLearning(n_components=n_comp,alpha=alpha,\n                                            batch_size=batch_size,n_iter=n_iter,random_state=rs)\n    elif(model_id is 'fastica'):\n        algorithm = 'parallel'; whiten = True; max_iter = 100\n        model = FastICA(n_components=n_comp, algorithm=algorithm,whiten=whiten, \n                          max_iter=max_iter, random_state=rs)\n    \n    # Scaling \n    if(scaler_id[0]):\n        \n        opts = [StandardScaler(),RobustScaler(),MinMaxScaler(), Normalizer(norm='l2')]\n        scaler = opts[scaler_id[1]].fit(X) \n        X_sca = pd.DataFrame(scaler.fit_transform(X),\n                                       columns = X.columns,\n                                       index = X.index) # summarize transformed data \n    \n    # Unsupervised Dimension Reduction \n    if(scaler_id[0]):\n        X_red = model.fit_transform(X_sca)\n    else:\n        X_red = model.fit_transform(X)\n    X_red = pd.DataFrame(data=X_red, index=X.index)\n    if(plot_id):\n        scatterPlot(X_red, y,model_id)\n    X_red[feature] = y\n    \n    return X_red # return new feature matrix","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-27T20:30:40.700299Z","iopub.execute_input":"2023-04-27T20:30:40.70063Z","iopub.status.idle":"2023-04-27T20:30:40.722718Z","shell.execute_reply.started":"2023-04-27T20:30:40.700598Z","shell.execute_reply":"2023-04-27T20:30:40.72128Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We use dimensionality reduction on <code>df_tr2</code>, before using the <code>modelEval</code> function as before, noting the accuracy & execution times\n- For dimensionality reduction, we'll use **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">Fast ICA</mark>** here together with different data **<mark style=\"background-color:#CDE10F;color:white;border-radius:5px;opacity:0.9\">data scaling methods</mark>**, you can try different combinations, with the aim being to obtain as high accuracy as possible","metadata":{}},{"cell_type":"code","source":"# Standard ICA (no scaling)\ndf_tr2_ICA = dimRed(df_tr2,\n                    split_id=[0.2,None],\n                    model_id='fastica',\n                    n_comp=5)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:30:40.724568Z","iopub.execute_input":"2023-04-27T20:30:40.725232Z","iopub.status.idle":"2023-04-27T20:30:43.849323Z","shell.execute_reply.started":"2023-04-27T20:30:40.725041Z","shell.execute_reply":"2023-04-27T20:30:43.84789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelEval(df_tr2_ICA,\n          split_id=[0.2,None],\n          plot_id=[False,True],\n          cv_yrange=(0.8,1.0),\n          hm_vvals=[0.8,1.0,0.9])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:30:43.851963Z","iopub.execute_input":"2023-04-27T20:30:43.852359Z","iopub.status.idle":"2023-04-27T20:32:04.002221Z","shell.execute_reply.started":"2023-04-27T20:30:43.852316Z","shell.execute_reply":"2023-04-27T20:32:04.000839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''StandardScaler ICA'''\ndf_tr2_ICA_sca0 = dimRed(df_tr2,\n                         split_id=[0.2,None],\n                         model_id='fastica',\n                         n_comp=5,\n                         scaler_id=[True,0])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:32:04.004093Z","iopub.execute_input":"2023-04-27T20:32:04.004636Z","iopub.status.idle":"2023-04-27T20:32:07.923735Z","shell.execute_reply.started":"2023-04-27T20:32:04.004581Z","shell.execute_reply":"2023-04-27T20:32:07.922485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelEval(df_tr2_ICA_sca0,\n          split_id=[0.2,None],\n          plot_id=[False,True],\n          cv_yrange=(0.8,1.0),\n          hm_vvals=[0.8,1.0,0.9])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:32:07.9251Z","iopub.execute_input":"2023-04-27T20:32:07.925546Z","iopub.status.idle":"2023-04-27T20:33:24.326712Z","shell.execute_reply.started":"2023-04-27T20:32:07.925508Z","shell.execute_reply":"2023-04-27T20:33:24.325235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''RobustScaler ICA'''\ndf_tr2_ICA_sca1 = dimRed(df_tr2,\n                         split_id=[0.2,None],\n                         model_id='fastica',\n                         n_comp=5,\n                         scaler_id=[True,1])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:33:24.328531Z","iopub.execute_input":"2023-04-27T20:33:24.329037Z","iopub.status.idle":"2023-04-27T20:33:27.135055Z","shell.execute_reply.started":"2023-04-27T20:33:24.32898Z","shell.execute_reply":"2023-04-27T20:33:27.133864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelEval(df_tr2_ICA_sca1,\n          split_id=[0.2,None],\n          plot_id=[False,True],\n          cv_yrange=(0.8,1.0),\n          hm_vvals=[0.8,1.0,0.9])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:33:27.136583Z","iopub.execute_input":"2023-04-27T20:33:27.137437Z","iopub.status.idle":"2023-04-27T20:34:43.026877Z","shell.execute_reply.started":"2023-04-27T20:33:27.137397Z","shell.execute_reply":"2023-04-27T20:34:43.025288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''MinMaxScaler ICA'''\ndf_tr2_ICA_sca2 = dimRed(df_tr2,\n                         split_id=[0.2,None],\n                         model_id='fastica',\n                         n_comp=5,\n                         scaler_id=[True,2])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:34:43.028487Z","iopub.execute_input":"2023-04-27T20:34:43.029336Z","iopub.status.idle":"2023-04-27T20:34:47.742587Z","shell.execute_reply.started":"2023-04-27T20:34:43.029295Z","shell.execute_reply":"2023-04-27T20:34:47.740952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelEval(df_tr2_ICA_sca2,\n          split_id=[0.2,None],\n          plot_id=[False,True],\n          cv_yrange=(0.8,1.0),\n          hm_vvals=[0.8,1.0,0.9])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:34:47.750956Z","iopub.execute_input":"2023-04-27T20:34:47.751749Z","iopub.status.idle":"2023-04-27T20:36:03.309518Z","shell.execute_reply.started":"2023-04-27T20:34:47.751701Z","shell.execute_reply":"2023-04-27T20:36:03.308487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Normaliser ICA'''\ndf_tr2_ICA_sca3 = dimRed(df_tr2,\n                         split_id=[0.2,None],\n                         model_id='fastica',\n                         n_comp=5,\n                         scaler_id=[True,3])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:36:03.311302Z","iopub.execute_input":"2023-04-27T20:36:03.311724Z","iopub.status.idle":"2023-04-27T20:36:11.356683Z","shell.execute_reply.started":"2023-04-27T20:36:03.311684Z","shell.execute_reply":"2023-04-27T20:36:11.355207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelEval(df_tr2_ICA_sca3,\n          split_id=[0.2,None],\n          plot_id=[False,True],\n          cv_yrange=(0.8,1.0),\n          hm_vvals=[0.8,1.0,0.9])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:36:11.358286Z","iopub.execute_input":"2023-04-27T20:36:11.359609Z","iopub.status.idle":"2023-04-27T20:37:27.59019Z","shell.execute_reply.started":"2023-04-27T20:36:11.359552Z","shell.execute_reply":"2023-04-27T20:37:27.589219Z"},"trusted":true},"execution_count":null,"outputs":[]}]}